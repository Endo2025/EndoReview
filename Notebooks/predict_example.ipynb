{"cells":[{"cell_type":"markdown","id":"e8a7fd94-75af-47d5-b20b-a330dca3eec6","metadata":{"id":"e8a7fd94-75af-47d5-b20b-a330dca3eec6","tags":[]},"source":["# Run Predict\n","\n","**Created on:** Monday, February 24, 2025, 08:36:14  \n","**Last modified:** February 2025\n","**Author:** anonymous author\n"]},{"cell_type":"markdown","id":"348bbd4f-f28d-4659-a885-be16af48123c","metadata":{"id":"348bbd4f-f28d-4659-a885-be16af48123c"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12zrP3bqw8FftpLJu1uWQ2nvvBKH6iJ_2)"]},{"cell_type":"markdown","id":"5a33490e-e4fe-40c0-ad12-9af958acf7c2","metadata":{"id":"5a33490e-e4fe-40c0-ad12-9af958acf7c2"},"source":["## Import Modules"]},{"cell_type":"code","source":["import sys\n","if \"google.colab\" in sys.modules:  # Solo ejecuta en Colab\n","    !apt-get update -qq > /dev/null\n","    !apt-get install --reinstall --yes libcudnn9 libcudnn9-dev -qq > /dev/null\n","    !pip uninstall -y torch torchvision > /dev/null 2>&1\n","    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124 > /dev/null 2>&1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-etTXIN8I3oI","executionInfo":{"status":"ok","timestamp":1742948527474,"user_tz":300,"elapsed":189418,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"8f124158-b444-4752-c4bb-1871ece38421"},"id":"-etTXIN8I3oI","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","E: Package 'libcudnn9' has no installation candidate\n","E: Package 'libcudnn9-dev' has no installation candidate\n"]}]},{"cell_type":"code","execution_count":2,"id":"63aa95c7-2be0-489c-95f9-781bd33b52d8","metadata":{"id":"63aa95c7-2be0-489c-95f9-781bd33b52d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742948552962,"user_tz":300,"elapsed":25495,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"2840b07a-1bdb-453c-f11f-dedb96bb0ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/823.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m819.2/823.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/960.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["import argparse\n","import sys\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","import cv2\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","try:\n","  import pytorch_lightning as pl\n","except:\n","  !pip install -q pytorch-lightning\n","  import pytorch_lightning as pl\n","\n","from tqdm import tqdm\n","\n","import timm\n","from IPython.display import display, HTML, Image as IPImage"]},{"cell_type":"code","source":["import torch\n","print(\"PyTorch Version:\", torch.__version__)\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","print(\"CUDA Version:\", torch.version.cuda)\n","!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsujvp4iHuzo","executionInfo":{"status":"ok","timestamp":1742948553052,"user_tz":300,"elapsed":93,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"62e88e87-edaa-41ad-9f4f-40121c1b8a05"},"id":"rsujvp4iHuzo","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version: 2.6.0+cu124\n","CUDA Available: True\n","CUDA Version: 12.4\n","#define CUDNN_MAJOR 9\n","#define CUDNN_MINOR 2\n","#define CUDNN_PATCHLEVEL 1\n","--\n","#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n","\n","/* cannot use constexpr here since this is a C-only file */\n"]}]},{"cell_type":"code","source":["import gdown\n","from typing import Dict\n","\n","def download_weights(weights_urls: Dict[str, str], output_paths: Dict[str, str]) -> None:\n","    \"\"\"\n","    Download model weights from Google Drive if they don't exist locally.\n","\n","    Args:\n","        weights_urls (Dict[str, str]): Dictionary of model names and their Google Drive download URLs\n","        output_paths (Dict[str, str]): Dictionary of model names and their local save paths\n","    \"\"\"\n","    for model_name, url in weights_urls.items():\n","        output_path = output_paths.get(model_name)\n","\n","        if not output_path:\n","            print(f\"No output path specified for {model_name}\")\n","            continue\n","\n","        try:\n","            if not os.path.exists(output_path):\n","                print(f\"Downloading {model_name} weights...\")\n","                gdown.download(url=url, output=output_path, quiet=False)\n","                print(f\"Successfully downloaded {model_name} weights to {output_path}\")\n","            else:\n","                print(f\"Weights for {model_name} already exist at {output_path}\")\n","\n","        except Exception as e:\n","            print(f\"Error downloading {model_name} weights: {e}\")\n","\n","def predict_to_dataframe(model, data_loader, df_to_save, device, column_name=\"organ_predict\"):\n","    \"\"\"\n","    Performs predictions using a model for all data in a loader and saves the results\n","    in a DataFrame.\n","\n","    Args:\n","        model: PyTorch model to perform predictions\n","        data_loader: PyTorch DataLoader with input data\n","        df_to_save: Pandas DataFrame where results will be saved\n","        device: Device where inference will be executed (CPU or GPU)\n","        column_name: Name of the column where predictions will be saved\n","\n","    Returns:\n","        DataFrame with saved predictions\n","    \"\"\"\n","    model.eval()  # Ensure the model is in evaluation mode\n","    counter = 0\n","\n","    # Iterate over all batches in the DataLoader with progress bar\n","    for batch in tqdm(data_loader, desc=\"Making predictions\"):\n","        # If batch is a tuple or list, take only x\n","        if isinstance(batch, tuple) or isinstance(batch, list):\n","            x = batch[0]\n","        else:\n","            x = batch\n","\n","        # Move data to the appropriate device\n","        x = x.to(device)\n","\n","        # Perform inference without calculating gradients\n","        with torch.no_grad():\n","            # Get model predictions\n","            output = model(x)\n","\n","            # Calculate predicted class\n","            y_pred = torch.argmax(output, dim=1)\n","\n","            # Process each sample in the batch\n","            batch_size = x.size(0)\n","            for i in range(batch_size):\n","                # Save prediction in the DataFrame\n","                df_to_save.loc[counter, column_name] = y_pred[i].item()\n","                counter += 1\n","\n","    return df_to_save\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224), interpolation=Image.LANCZOS),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5990, 0.3664, 0.2769], [0.2847, 0.2190, 0.1772])\n","])\n","map_organ = {0:'pharynx',1:'esophagus',2:'stomach',3:'duodenum'}\n","map_stomach = {0:'A1',1:'L1',2:'P1',3:'G1',4:'A2',5:'L2',6:'P2',7:'G2',8:'A3',9:'L3',10:'P3',11:'G3',12:'A4',13:'L4',14:'P4',15:'G4',16:'A5',17:'L5',18:'P5',19:'A6',20:'L6',21:'P6', 22:'No quality'}"],"metadata":{"id":"DO1igfWR22gL","executionInfo":{"status":"ok","timestamp":1742948553537,"user_tz":300,"elapsed":482,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}}},"id":"DO1igfWR22gL","execution_count":4,"outputs":[]},{"cell_type":"markdown","id":"d0795e16-e92c-4cc7-ba4e-9c1ff8489ca0","metadata":{"id":"d0795e16-e92c-4cc7-ba4e-9c1ff8489ca0"},"source":["## Downloading Trained Weights\n","\n"]},{"cell_type":"code","source":["WEIGHTS_URLS = {\n","    'data_manager': \"https://drive.google.com/uc?id=14ZmGnZs_Q-0pAcTPpRYItajAzZlnvvbQ\",\n","    'trained_model_loader': \"https://drive.google.com/uc?id=1eggzPzsYQSsrnGeAQlY52Pgo4ybMGkg1\",\n","    'video': \"https://drive.google.com/uc?id=1rmKKgonVazJxtDeOIXzUKW3y5Szx6hsV\",\n","    'embedding': \"https://drive.google.com/uc?id=1bAkdxB2_uR6oXYZNvxOSUAAHpHkSHSg1\",\n","    'organ_model': \"https://drive.google.com/uc?id=1pxcgpFmbIglPX99n-GPWrvh6lOP7T5l2\",\n","    'stomach_model': \"https://drive.google.com/uc?id=1PCVgzHVEb8UkB3FPMGt-qwHgNNj_TVrc\"\n","}\n","\n","OUTPUT_PATHS = {\n","    'data_manager': 'data_manager.py',\n","    'trained_model_loader': 'trained_model_loader.py',\n","    'video': 'video.mp4',\n","    'embedding': 'embedding.ckpt',\n","    'organ_model': 'organ_model.ckpt',\n","    'stomach_model': 'stomach_model.ckpt'\n","}\n","\n","# Call the function to download weights\n","download_weights(WEIGHTS_URLS, OUTPUT_PATHS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxmlY9j-2xgh","executionInfo":{"status":"ok","timestamp":1742948579847,"user_tz":300,"elapsed":26308,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"870b840f-8aa9-4e83-8da7-f0edef4b3020"},"id":"rxmlY9j-2xgh","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data_manager weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=14ZmGnZs_Q-0pAcTPpRYItajAzZlnvvbQ\n","From (redirected): https://drive.google.com/uc?id=14ZmGnZs_Q-0pAcTPpRYItajAzZlnvvbQ&confirm=t&uuid=f18a728b-66ed-4382-a6a1-052c829294b5\n","To: /content/data_manager.py\n","100%|██████████| 7.24k/7.24k [00:00<00:00, 4.90MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded data_manager weights to data_manager.py\n","Downloading trained_model_loader weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1eggzPzsYQSsrnGeAQlY52Pgo4ybMGkg1\n","From (redirected): https://drive.google.com/uc?id=1eggzPzsYQSsrnGeAQlY52Pgo4ybMGkg1&confirm=t&uuid=7b3d0caa-b5a8-484c-a90a-9d47814be5d3\n","To: /content/trained_model_loader.py\n","100%|██████████| 4.76k/4.76k [00:00<00:00, 7.10MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded trained_model_loader weights to trained_model_loader.py\n","Downloading video weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1rmKKgonVazJxtDeOIXzUKW3y5Szx6hsV\n","To: /content/video.mp4\n","100%|██████████| 2.88M/2.88M [00:00<00:00, 92.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded video weights to video.mp4\n","Downloading embedding weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1bAkdxB2_uR6oXYZNvxOSUAAHpHkSHSg1\n","From (redirected): https://drive.google.com/uc?id=1bAkdxB2_uR6oXYZNvxOSUAAHpHkSHSg1&confirm=t&uuid=b86d27bb-c8b3-4396-b8a7-2e71c081e94e\n","To: /content/embedding.ckpt\n","100%|██████████| 111M/111M [00:00<00:00, 162MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded embedding weights to embedding.ckpt\n","Downloading organ_model weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1pxcgpFmbIglPX99n-GPWrvh6lOP7T5l2\n","From (redirected): https://drive.google.com/uc?id=1pxcgpFmbIglPX99n-GPWrvh6lOP7T5l2&confirm=t&uuid=4677c32f-0f4f-40b7-854d-b634765c1e38\n","To: /content/organ_model.ckpt\n","100%|██████████| 341M/341M [00:01<00:00, 233MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded organ_model weights to organ_model.ckpt\n","Downloading stomach_model weights...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1PCVgzHVEb8UkB3FPMGt-qwHgNNj_TVrc\n","From (redirected): https://drive.google.com/uc?id=1PCVgzHVEb8UkB3FPMGt-qwHgNNj_TVrc&confirm=t&uuid=3c011c39-d862-4868-a32c-a3410e8496f7\n","To: /content/stomach_model.ckpt\n","100%|██████████| 341M/341M [00:01<00:00, 262MB/s]"]},{"output_type":"stream","name":"stdout","text":["Successfully downloaded stomach_model weights to stomach_model.ckpt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":6,"id":"da1d499b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da1d499b","executionInfo":{"status":"ok","timestamp":1742948579900,"user_tz":300,"elapsed":51,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"409a1da9-6893-4ee7-b746-a4d820474708"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYDEVD_DISABLE_FILE_VALIDATION=1\n"]}],"source":["# load modules\n","from data_manager import VideoFrameDataset, load_frames_from_video, get_sequences_space, expand_sequences, CustomDataset\n","from trained_model_loader import initialize_model, Identity, ModelInference, load_model_for_inference, load_vit_model_for_inference\n","%env PYDEVD_DISABLE_FILE_VALIDATION=1"]},{"cell_type":"code","source":["#======================================\n","# Get and set all input parameters\n","#======================================\n","def get_args_parser():\n","    parser = argparse.ArgumentParser('Predict', add_help=False)\n","    # Video path\n","    parser.add_argument('--path_video', type=str, default=os.path.join(\"video.mp4\"),\n","                        help='path image cnn embedding')\n","    # Model paths\n","    parser.add_argument('--path_image_embedding', type=str, default=os.path.join(\"embedding.ckpt\"),\n","                        help='path image cnn embedding')\n","    parser.add_argument('--path_organ_classification', type=str, default=os.path.join(\"organ_model.ckpt\"),\n","                        help='path organ transformer classification')\n","    parser.add_argument('--path_stomach_classification', type=str, default=os.path.join(\"stomach_model.ckpt\"),\n","                        help='path stomach transformer classification')\n","    # Model parameters\n","    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of workers in dataloader\")\n","    parser.add_argument('--batch_size',type=int, default=16, help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus') # 1024\n","\n","    return parser"],"metadata":{"id":"eV79SK6v4tkr","executionInfo":{"status":"ok","timestamp":1742948579901,"user_tz":300,"elapsed":24,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}}},"id":"eV79SK6v4tkr","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"SBn9zymPEg06"},"id":"SBn9zymPEg06"},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = get_args_parser()\n","    args, unknown = parser.parse_known_args()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    #==========================================\n","    # Embedding Representation\n","    #==========================================\n","    # Load the model\n","    # a) Load the backbone\n","    model_ft, CNN_family = initialize_model(\"convnext_tiny\", 23, True, (224, 224))\n","    for param in model_ft.parameters():\n","        param.requires_grad = False\n","    # b) Load the weights\n","    model = load_model_for_inference(args.path_image_embedding, model_ft, device=device)\n","    # c) Preprocessing\n","    #### Add functions to crop frame: roi\n","    # d) get frame rate\n","    cap = cv2.VideoCapture(args.path_video)\n","    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if not cap.isOpened():\n","        print(f\"❌ Error: Could not open video {args.path_video}\")\n","    frames_tensor = load_frames_from_video(cap, model, transform, device)\n","    cap.release()\n","    #==========================================\n","    # Dataloader\n","    #==========================================\n","    MAX_SEQ_LENGTH = 135 # temporal window trained\n","    df_setup_expanded = expand_sequences(num_frames,MAX_SEQ_LENGTH)\n","    assert num_frames == len(df_setup_expanded)\n","    # Define dataset\n","    dataset = CustomDataset(df_setup_expanded, frames_tensor)\n","    # Define DataLoader\n","    data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n","\n","    #####################################\n","    # 1) Organ Classification\n","    # ###################################\n","    # a) Cargar modelo para inferencia\n","    vit_model_organ = load_vit_model_for_inference(args.path_organ_classification, device, num_classes=4, max_seq_length=MAX_SEQ_LENGTH)\n","    print(\"Modelo ViT Organ cargado exitosamente para inferencia.\")\n","    # b) Inferencia Organ\n","    df_setup_expanded[\"organ_predict\"] = None\n","    df_setup_expanded = predict_to_dataframe(\n","        model=vit_model_organ,\n","        data_loader=data_loader,\n","        df_to_save=df_setup_expanded,\n","        device=device,\n","        column_name=\"organ_predict\"\n","    )\n","    #####################################\n","    # 2) Stomach Site Classification\n","    #####################################\n","    # a) Cargar modelo para inferencia\n","    vit_model_organ = load_vit_model_for_inference(args.path_stomach_classification, device, num_classes=23, max_seq_length=MAX_SEQ_LENGTH)\n","    print(\"Modelo ViT cargado exitosamente para inferencia.\")\n","    # b) Inferencia Organ\n","    df_setup_expanded[\"stomach_predict\"] = None\n","    df_setup_expanded = predict_to_dataframe(\n","        model=vit_model_organ,\n","        data_loader=data_loader,\n","        df_to_save=df_setup_expanded,\n","        device=device,\n","        column_name=\"stomach_predict\"\n","    )\n","    # map\n","    roi_index = df_setup_expanded[df_setup_expanded[\"organ_predict\"] != 2].index\n","    df_setup_expanded.loc[roi_index,\"stomach_predict\"] = \"No stomach\"\n","    df_setup_expanded[\"organ_predict\"]=df_setup_expanded[\"organ_predict\"].replace(map_organ)\n","    df_setup_expanded[\"stomach_predict\"]=df_setup_expanded[\"stomach_predict\"].replace(map_stomach)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avczV7Pr5L_B","executionInfo":{"status":"ok","timestamp":1742948595723,"user_tz":300,"elapsed":15826,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"00f556fe-561b-4919-8c4a-583d81c33de7"},"id":"avczV7Pr5L_B","execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n","100%|██████████| 109M/109M [00:00<00:00, 116MB/s]\n","Processing Frames: 100%|██████████| 152/152 [00:06<00:00, 25.24frame/s]\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Modelo ViT Organ cargado exitosamente para inferencia.\n"]},{"output_type":"stream","name":"stderr","text":["\rMaking predictions:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Making predictions: 100%|██████████| 10/10 [00:01<00:00,  6.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Modelo ViT cargado exitosamente para inferencia.\n"]},{"output_type":"stream","name":"stderr","text":["\rMaking predictions:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Making predictions: 100%|██████████| 10/10 [00:01<00:00,  6.59it/s]\n"]}]},{"cell_type":"code","source":["# Open the video file\n","cap = cv2.VideoCapture(args.path_video)\n","frames_selected = []\n","\n","# Current frame index\n","current_frame = 0\n","\n","while True:\n","    # Read a frame from the video\n","    ret, frame = cap.read()\n","    # If no more frames, exit the loop\n","    if not ret:\n","        break\n","    frame = cv2.resize(frame, (224, 224))\n","    # Find the corresponding row in df_setup_expanded for the current frame\n","    frame_row = df_setup_expanded[df_setup_expanded['frame_roi'] == current_frame]\n","    if not frame_row.empty:\n","        # Get predictions\n","        organ_predict = frame_row['organ_predict'].values[0]\n","        stomach_predict = frame_row['stomach_predict'].values[0]\n","        # Configure text for predictions\n","        organ_text = f\"Organ Pred.: {organ_predict}\"\n","        stomach_text = f\"Stomach Pred.: {stomach_predict}\"\n","        # Get frame dimensions\n","        height, width = frame.shape[:2]\n","        # Add text in the central part of the frame\n","        cv2.putText(frame,organ_text,(0, height // 2),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255, 255, 255),2)\n","        # Add text in the bottom part of the frame\n","        cv2.putText(frame, stomach_text, (0, height - 50),cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255, 255),2)\n","        # Convert frame to PIL Image and add to selected frames\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frames_selected.append(Image.fromarray(frame))\n","    # Increment frame counter\n","    current_frame += 1\n","# Release video capture resources\n","cap.release()\n","# Define output GIF path\n","gif_path = 'output.gif'\n","# Save selected frames as an animated GIF\n","frames_selected[0].save(gif_path, save_all=True, append_images=frames_selected[1:], duration=100, loop=0)"],"metadata":{"id":"5OrWAi9M-hpr","executionInfo":{"status":"ok","timestamp":1742948600841,"user_tz":300,"elapsed":5116,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}}},"id":"5OrWAi9M-hpr","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Display the GIF\n","print(\"Result\")\n","display(IPImage(filename=gif_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258,"output_embedded_package_id":"15APxiRG3hkms9hI1dG6ukCCj64Z3RQpY"},"id":"8RBYZyKS_fF6","executionInfo":{"status":"ok","timestamp":1742948602760,"user_tz":300,"elapsed":1913,"user":{"displayName":"Cuenta Grupo Cimalab","userId":"02751550196831432892"}},"outputId":"8ed3c3ad-bfdc-4f40-d0cc-d06cd503c51e"},"id":"8RBYZyKS_fF6","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":5}